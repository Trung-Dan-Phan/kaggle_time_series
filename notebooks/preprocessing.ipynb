{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install meteostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qolmat\n",
      "  Downloading qolmat-0.1.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting category-encoders (from qolmat)\n",
      "  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting dcor>=0.6 (from qolmat)\n",
      "  Downloading dcor-0.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting hyperopt (from qolmat)\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from qolmat) (2.1.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from qolmat) (24.1)\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from qolmat) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from qolmat) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from qolmat) (1.14.1)\n",
      "Collecting statsmodels>=0.14 (from qolmat)\n",
      "  Downloading statsmodels-0.14.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from qolmat) (4.12.2)\n",
      "Collecting numba>=0.51 (from dcor>=0.6->qolmat)\n",
      "  Downloading numba-0.60.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from dcor>=0.6->qolmat) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from pandas>=1.3->qolmat) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from pandas>=1.3->qolmat) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from pandas>=1.3->qolmat) (2024.2)\n",
      "Collecting patsy>=0.5.6 (from statsmodels>=0.14->qolmat)\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from scikit-learn->qolmat) (3.5.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from hyperopt->qolmat) (1.16.0)\n",
      "Collecting networkx>=2.2 (from hyperopt->qolmat)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting future (from hyperopt->qolmat)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/time_series/lib/python3.10/site-packages (from hyperopt->qolmat) (4.66.6)\n",
      "Collecting cloudpickle (from hyperopt->qolmat)\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting py4j (from hyperopt->qolmat)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.51->dcor>=0.6->qolmat)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting numpy>=1.21 (from qolmat)\n",
      "  Downloading numpy-2.0.2-cp310-cp310-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Downloading qolmat-0.1.8-py3-none-any.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dcor-0.6-py3-none-any.whl (55 kB)\n",
      "Downloading statsmodels-0.14.4-cp310-cp310-macosx_11_0_arm64.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
      "Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.60.0-cp310-cp310-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m992.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Downloading llvmlite-0.43.0-cp310-cp310-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m945.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: py4j, numpy, networkx, llvmlite, future, cloudpickle, patsy, numba, statsmodels, hyperopt, dcor, category-encoders, qolmat\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed category-encoders-2.6.4 cloudpickle-3.1.0 dcor-0.6 future-1.0.0 hyperopt-0.2.7 llvmlite-0.43.0 networkx-3.4.2 numba-0.60.0 numpy-2.0.2 patsy-0.5.6 py4j-0.10.9.7 qolmat-0.1.8 statsmodels-0.14.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install qolmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from qolmat.imputations import imputers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"https://raw.githubusercontent.com/Trung-Dan-Phan/kaggle_time_series/refs/heads/eda/data/test.csv\"\n",
    "train_path = \"https://raw.githubusercontent.com/Trung-Dan-Phan/kaggle_time_series/refs/heads/eda/data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "\n",
    "#### Date and Time Features\n",
    "- [ ] **Extract Date Features**: Extract year, month, day, and hour from the timestamp for each entry.\n",
    "- [ ] **Cyclic Features**: Add cyclic features for hour and month using sine and cosine transformations.\n",
    "\n",
    "#### Lag and Rolling Features\n",
    "- [ ] **Lagged Values**: Create lagged features for each pollutant (1, 24, 168 hours to capture short, daily, and weekly patterns).\n",
    "- [ ] **Rolling Statistics**: Add rolling mean, rolling standard deviation, and rolling min/max features to smooth trends.\n",
    "\n",
    "#### Handle Missing Values\n",
    "- [ ] **Impute Missing Values**: Choose and implement a strategy (e.g., forward-fill, backward-fill, interpolation) for missing data in each pollutant.\n",
    "- [ ] **Drop or Flag Missing Rows**: If a significant portion of the data is missing, consider dropping those rows or creating an indicator feature to mark them.\n",
    "\n",
    "#### External Weather Data (Optional but Recommended)\n",
    "- [ ] **Incorporate Weather Data**: If available, obtain weather data for Paris (temperature, humidity, wind speed).\n",
    "- [ ] **Merge with Pollutant Data**: Align weather data with your time series dataset based on the timestamp.\n",
    "\n",
    "#### Data Scaling\n",
    "- [ ] **Scale Features**: Use standard scaling or min-max scaling for numerical features, especially for pollutants and weather variables (if any).\n",
    "\n",
    "#### Target Preparation\n",
    "- [ ] **Prepare Target Variables**: Shift the pollutant columns to create target variables aligned with the forecasting horizon (e.g., next hour, next day).\n",
    "- [ ] **Split Data**: Divide the data into training and validation sets, ensuring the split respects the time sequence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix the missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer to [qolmat, tsa](https://qolmat.readthedocs.io/en/latest/generated/qolmat.imputations.imputers.ImputerInterpolation.html#qolmat.imputations.imputers.ImputerInterpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "# Replace inf/-inf with NaN (if any)\n",
    "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Impute missing values (using median as it's robust to outliers)\n",
    "#imputer = SimpleImputer(strategy='median')\n",
    "#train_df = pd.DataFrame(imputer.fit_transform(train_df), columns=train_df.columns, index=train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>valeur_NO2</th>\n",
       "      <th>valeur_CO</th>\n",
       "      <th>valeur_O3</th>\n",
       "      <th>valeur_PM10</th>\n",
       "      <th>valeur_PM25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.718</td>\n",
       "      <td>15.7</td>\n",
       "      <td>73.1</td>\n",
       "      <td>64.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.587</td>\n",
       "      <td>10.1</td>\n",
       "      <td>74.8</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  valeur_NO2  valeur_CO  valeur_O3  valeur_PM10  valeur_PM25\n",
       "0  2020-01-01 00        42.9      0.718       15.7         73.1         64.4\n",
       "1  2020-01-01 01        33.6      0.587       10.1         74.8         66.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the best way filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10',\n",
    "       'valeur_PM25']\n",
    "def check_filling_result(df=train_df, fixed_df=\"\"):\n",
    "    for col in cols:\n",
    "        plt.figure(figsize=(20, 12))\n",
    "\n",
    "        # First subplot in blue\n",
    "        plt.subplot(2, 1, 1)  # 2 rows, 1 column, 1st plot\n",
    "        train_df[col].plot(kind='line', color='blue', title=f\"Train Data - Column '{col}' (Blue)\")\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"Value\")\n",
    "\n",
    "        # Second subplot in red\n",
    "        plt.subplot(2, 1, 2)  # 2 rows, 1 column, 2nd plot\n",
    "        fixed_df[col].plot(kind='line', color='red', title=\"Fixed Train Data - Column '{col}' (Red)\")\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"Value\")\n",
    "\n",
    "        # Display the plots\n",
    "        plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should directly cut some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the DataFrame is already loaded and named `df`\n",
    "# Sample structure based on the image:\n",
    "# df = pd.read_csv('your_file.csv')  # Replace with your actual file loading method\n",
    "\n",
    "# Create empty lists to store results\n",
    "def count_consecutive_nulls(df=train_df, date_col = 'id', target_col=''):\n",
    "    start_dates = []\n",
    "    end_dates = []\n",
    "    null_counts = []\n",
    "\n",
    "    # Track consecutive nulls\n",
    "    null_count = 0\n",
    "    start_date = None\n",
    "\n",
    "    # Iterate over the DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isna(row[target_col]):\n",
    "            # If null, increase the count and set the start date if it's the first in the sequence\n",
    "            if null_count == 0:\n",
    "                start_date = row[date_col]  # Store the start date of the null sequence\n",
    "            null_count += 1\n",
    "            end_date = row[date_col]  # Continuously update end date for each null in the sequence\n",
    "        else:\n",
    "            # If not null and there was a sequence of nulls, store the result\n",
    "            if null_count > 0:\n",
    "                start_dates.append(start_date)\n",
    "                end_dates.append(end_date)\n",
    "                null_counts.append(null_count)\n",
    "                # Reset count and start date\n",
    "                null_count = 0\n",
    "                start_date = None\n",
    "\n",
    "    # Check if there's an ongoing sequence at the end of the loop\n",
    "    if null_count > 0:\n",
    "        start_dates.append(start_date)\n",
    "        end_dates.append(end_date)\n",
    "        null_counts.append(null_count)\n",
    "\n",
    "    # Compile the result into a dictionary\n",
    "    result = {\"start_date\": start_dates, \"end_date\": end_dates, \"null_num\": null_counts}\n",
    "\n",
    "    # Convert to DataFrame for easier visualization if needed\n",
    "    result_df = pd.DataFrame(result)\n",
    "    # Convert to DataFrame for easier visualization if needed\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df = result_df.sort_values(by='null_num', ascending=False)\n",
    "    result_df['percentage'] = result_df['null_num']/df.shape[0]*100\n",
    "    print(result_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       start_date       end_date  null_num  percentage\n",
      "56  2024-07-31 15  2024-08-14 09       331    1.669778\n",
      "62  2024-09-01 02  2024-09-03 22        69    0.348081\n",
      "50  2024-06-25 18  2024-06-26 22        29    0.146295\n",
      "10  2022-10-17 23  2022-10-18 23        25    0.126116\n",
      "59  2024-08-26 13  2024-08-27 10        22    0.110982\n",
      "       start_date       end_date  null_num  percentage\n",
      "6   2022-09-15 17  2022-09-26 10       258    1.301518\n",
      "19  2023-08-11 18  2023-08-16 11       114    0.575090\n",
      "18  2023-07-14 01  2023-07-17 07        79    0.398527\n",
      "0   2022-06-28 12  2022-06-29 12        25    0.126116\n",
      "23  2024-02-22 13  2024-02-23 10        22    0.110982\n",
      "       start_date       end_date  null_num  percentage\n",
      "12  2023-04-13 08  2023-04-14 12        29    0.146295\n",
      "23  2024-02-22 13  2024-02-23 10        22    0.110982\n",
      "27  2024-07-03 13  2024-07-04 07        19    0.095848\n",
      "5   2022-09-13 15  2022-09-14 08        18    0.090804\n",
      "13  2023-04-27 14  2023-04-28 06        17    0.085759\n",
      "        start_date       end_date  null_num  percentage\n",
      "33   2022-07-13 06  2022-07-16 23        90    0.454018\n",
      "30   2022-07-07 06  2022-07-10 18        85    0.428795\n",
      "248  2023-09-08 05  2023-09-10 22        66    0.332947\n",
      "34   2022-07-17 06  2022-07-19 01        44    0.221964\n",
      "202  2023-07-10 06  2023-07-12 01        44    0.221964\n",
      "       start_date       end_date  null_num  percentage\n",
      "50  2024-02-22 13  2024-02-23 10        22    0.110982\n",
      "10  2022-09-13 15  2022-09-14 08        18    0.090804\n",
      "24  2023-04-27 14  2023-04-28 06        17    0.085759\n",
      "64  2024-08-28 19  2024-08-29 06        12    0.060536\n",
      "5   2022-08-01 21  2022-08-02 07        11    0.055491\n"
     ]
    }
   ],
   "source": [
    "for target in cols:\n",
    "    print(f'{target}')\n",
    "    count_consecutive_nulls(target_col=target)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only keep the data after 2022 june 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valeur_NO2\n",
      "       start_date       end_date  null_num  percentage\n",
      "56  2024-07-31 15  2024-08-14 09       331    1.669778\n",
      "62  2024-09-01 02  2024-09-03 22        69    0.348081\n",
      "50  2024-06-25 18  2024-06-26 22        29    0.146295\n",
      "10  2022-10-17 23  2022-10-18 23        25    0.126116\n",
      "59  2024-08-26 13  2024-08-27 10        22    0.110982\n",
      "\n",
      "\n",
      "valeur_CO\n",
      "       start_date       end_date  null_num  percentage\n",
      "6   2022-09-15 17  2022-09-26 10       258    1.301518\n",
      "19  2023-08-11 18  2023-08-16 11       114    0.575090\n",
      "18  2023-07-14 01  2023-07-17 07        79    0.398527\n",
      "0   2022-06-28 12  2022-06-29 12        25    0.126116\n",
      "23  2024-02-22 13  2024-02-23 10        22    0.110982\n",
      "\n",
      "\n",
      "valeur_O3\n",
      "       start_date       end_date  null_num  percentage\n",
      "12  2023-04-13 08  2023-04-14 12        29    0.146295\n",
      "23  2024-02-22 13  2024-02-23 10        22    0.110982\n",
      "27  2024-07-03 13  2024-07-04 07        19    0.095848\n",
      "5   2022-09-13 15  2022-09-14 08        18    0.090804\n",
      "13  2023-04-27 14  2023-04-28 06        17    0.085759\n",
      "\n",
      "\n",
      "valeur_PM10\n",
      "        start_date       end_date  null_num  percentage\n",
      "33   2022-07-13 06  2022-07-16 23        90    0.454018\n",
      "30   2022-07-07 06  2022-07-10 18        85    0.428795\n",
      "248  2023-09-08 05  2023-09-10 22        66    0.332947\n",
      "34   2022-07-17 06  2022-07-19 01        44    0.221964\n",
      "202  2023-07-10 06  2023-07-12 01        44    0.221964\n",
      "\n",
      "\n",
      "valeur_PM25\n",
      "       start_date       end_date  null_num  percentage\n",
      "50  2024-02-22 13  2024-02-23 10        22    0.110982\n",
      "10  2022-09-13 15  2022-09-14 08        18    0.090804\n",
      "24  2023-04-27 14  2023-04-28 06        17    0.085759\n",
      "64  2024-08-28 19  2024-08-29 06        12    0.060536\n",
      "5   2022-08-01 21  2022-08-02 07        11    0.055491\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.loc[pd.to_datetime(train_df['id'])>='2022-06-01']\n",
    "for target in cols:\n",
    "    print(target)\n",
    "    count_consecutive_nulls(target_col=target)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the best filling methods using qolmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qolmat.benchmark import comparator, missing_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: interpolation...done.\n",
      "Testing model: residual...done.\n",
      "Testing model: mce...done.\n",
      "Testing model: em...done.\n",
      "Testing model: locf...done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_614e1_row0_col3, #T_614e1_row1_col2, #T_614e1_row2_col1, #T_614e1_row3_col3, #T_614e1_row4_col1 {\n",
       "  background-color: lightsteelblue;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_614e1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_614e1_level0_col0\" class=\"col_heading level0 col0\" >interpolation</th>\n",
       "      <th id=\"T_614e1_level0_col1\" class=\"col_heading level0 col1\" >residual</th>\n",
       "      <th id=\"T_614e1_level0_col2\" class=\"col_heading level0 col2\" >mce</th>\n",
       "      <th id=\"T_614e1_level0_col3\" class=\"col_heading level0 col3\" >em</th>\n",
       "      <th id=\"T_614e1_level0_col4\" class=\"col_heading level0 col4\" >locf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_614e1_level0_row0\" class=\"row_heading level0 row0\" rowspan=\"5\">mae</th>\n",
       "      <th id=\"T_614e1_level1_row0\" class=\"row_heading level1 row0\" >valeur_NO2</th>\n",
       "      <td id=\"T_614e1_row0_col0\" class=\"data row0 col0\" >8.503520</td>\n",
       "      <td id=\"T_614e1_row0_col1\" class=\"data row0 col1\" >9.413740</td>\n",
       "      <td id=\"T_614e1_row0_col2\" class=\"data row0 col2\" >6.046929</td>\n",
       "      <td id=\"T_614e1_row0_col3\" class=\"data row0 col3\" >6.005271</td>\n",
       "      <td id=\"T_614e1_row0_col4\" class=\"data row0 col4\" >11.836642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_614e1_level1_row1\" class=\"row_heading level1 row1\" >valeur_CO</th>\n",
       "      <td id=\"T_614e1_row1_col0\" class=\"data row1 col0\" >0.044426</td>\n",
       "      <td id=\"T_614e1_row1_col1\" class=\"data row1 col1\" >0.052599</td>\n",
       "      <td id=\"T_614e1_row1_col2\" class=\"data row1 col2\" >0.038915</td>\n",
       "      <td id=\"T_614e1_row1_col3\" class=\"data row1 col3\" >0.055543</td>\n",
       "      <td id=\"T_614e1_row1_col4\" class=\"data row1 col4\" >0.063759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_614e1_level1_row2\" class=\"row_heading level1 row2\" >valeur_O3</th>\n",
       "      <td id=\"T_614e1_row2_col0\" class=\"data row2 col0\" >11.228273</td>\n",
       "      <td id=\"T_614e1_row2_col1\" class=\"data row2 col1\" >10.280442</td>\n",
       "      <td id=\"T_614e1_row2_col2\" class=\"data row2 col2\" >15.638046</td>\n",
       "      <td id=\"T_614e1_row2_col3\" class=\"data row2 col3\" >15.918619</td>\n",
       "      <td id=\"T_614e1_row2_col4\" class=\"data row2 col4\" >15.295825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_614e1_level1_row3\" class=\"row_heading level1 row3\" >valeur_PM10</th>\n",
       "      <td id=\"T_614e1_row3_col0\" class=\"data row3 col0\" >4.210921</td>\n",
       "      <td id=\"T_614e1_row3_col1\" class=\"data row3 col1\" >4.298318</td>\n",
       "      <td id=\"T_614e1_row3_col2\" class=\"data row3 col2\" >3.451695</td>\n",
       "      <td id=\"T_614e1_row3_col3\" class=\"data row3 col3\" >3.235518</td>\n",
       "      <td id=\"T_614e1_row3_col4\" class=\"data row3 col4\" >5.851337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_614e1_level1_row4\" class=\"row_heading level1 row4\" >valeur_PM25</th>\n",
       "      <td id=\"T_614e1_row4_col0\" class=\"data row4 col0\" >1.875342</td>\n",
       "      <td id=\"T_614e1_row4_col1\" class=\"data row4 col1\" >1.745852</td>\n",
       "      <td id=\"T_614e1_row4_col2\" class=\"data row4 col2\" >2.535118</td>\n",
       "      <td id=\"T_614e1_row4_col3\" class=\"data row4 col3\" >2.664977</td>\n",
       "      <td id=\"T_614e1_row4_col4\" class=\"data row4 col4\" >2.423512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1327ffd90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# impute and compare\n",
    "imputer_interpol = imputers.ImputerInterpolation(method=\"linear\")\n",
    "imputer_res = imputers.ImputerResiduals(period=24, model_tsa=\"additive\")\n",
    "# imputer_rpca = imputers.ImputerRpcaNoisy()\n",
    "imputer_mce = imputers.ImputerMICE()\n",
    "imputer_em = imputers.ImputerEM(columnwise=False)\n",
    "imputer_locf = imputers.ImputerLOCF()\n",
    "\n",
    "dict_imputers = {\n",
    "      \"interpolation\": imputer_interpol,\n",
    "      \"residual\": imputer_res,\n",
    "      #\"rpca\": imputer_rpca,\n",
    "      \"mce\": imputer_mce,\n",
    "      \"em\": imputer_em,\n",
    "      \"locf\": imputer_locf,\n",
    "  }\n",
    "\n",
    "generator_holes = missing_patterns.EmpiricalHoleGenerator(n_splits=4, ratio_masked=0.1)\n",
    "comparison = comparator.Comparator(\n",
    "      dict_imputers,\n",
    "      cols,\n",
    "      generator_holes = generator_holes,\n",
    "      metrics = [\"mae\"],\n",
    "  )\n",
    "results = comparison.compare(train_df)\n",
    "results.style.highlight_min(color=\"lightsteelblue\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion:  \n",
    "NO2: em  \n",
    "CO: mce  \n",
    "O3: residual  \n",
    "PM10: em  \n",
    "PM25: residual  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>valeur_NO2</th>\n",
       "      <th>valeur_CO</th>\n",
       "      <th>valeur_O3</th>\n",
       "      <th>valeur_PM10</th>\n",
       "      <th>valeur_PM25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21168</th>\n",
       "      <td>2022-06-01 00</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.148</td>\n",
       "      <td>67.4</td>\n",
       "      <td>10.2</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21169</th>\n",
       "      <td>2022-06-01 01</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.140</td>\n",
       "      <td>71.9</td>\n",
       "      <td>15.7</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21170</th>\n",
       "      <td>2022-06-01 02</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.140</td>\n",
       "      <td>69.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21171</th>\n",
       "      <td>2022-06-01 03</td>\n",
       "      <td>24.9</td>\n",
       "      <td>0.155</td>\n",
       "      <td>51.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21172</th>\n",
       "      <td>2022-06-01 04</td>\n",
       "      <td>35.2</td>\n",
       "      <td>0.190</td>\n",
       "      <td>38.2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40986</th>\n",
       "      <td>2024-09-03 18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222</td>\n",
       "      <td>55.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40987</th>\n",
       "      <td>2024-09-03 19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.245</td>\n",
       "      <td>48.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40988</th>\n",
       "      <td>2024-09-03 20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234</td>\n",
       "      <td>44.5</td>\n",
       "      <td>12.4</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40989</th>\n",
       "      <td>2024-09-03 21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.225</td>\n",
       "      <td>25.9</td>\n",
       "      <td>10.6</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40990</th>\n",
       "      <td>2024-09-03 22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.184</td>\n",
       "      <td>37.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19823 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  valeur_NO2  valeur_CO  valeur_O3  valeur_PM10  \\\n",
       "21168  2022-06-01 00        16.4      0.148       67.4         10.2   \n",
       "21169  2022-06-01 01        12.3      0.140       71.9         15.7   \n",
       "21170  2022-06-01 02        12.1      0.140       69.2          8.4   \n",
       "21171  2022-06-01 03        24.9      0.155       51.0         11.8   \n",
       "21172  2022-06-01 04        35.2      0.190       38.2         16.0   \n",
       "...              ...         ...        ...        ...          ...   \n",
       "40986  2024-09-03 18         NaN      0.222       55.1         12.0   \n",
       "40987  2024-09-03 19         NaN      0.245       48.2         13.4   \n",
       "40988  2024-09-03 20         NaN      0.234       44.5         12.4   \n",
       "40989  2024-09-03 21         NaN      0.225       25.9         10.6   \n",
       "40990  2024-09-03 22         NaN      0.184       37.7          8.5   \n",
       "\n",
       "       valeur_PM25  \n",
       "21168          4.1  \n",
       "21169          5.2  \n",
       "21170          3.5  \n",
       "21171          4.9  \n",
       "21172          5.8  \n",
       "...            ...  \n",
       "40986          5.3  \n",
       "40987          7.0  \n",
       "40988          7.1  \n",
       "40989          5.4  \n",
       "40990          4.6  \n",
       "\n",
       "[19823 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['id', '']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling/Normalization\n",
    "\n",
    "# Select features for scaling (all pollutant columns and lags)\n",
    "scaler = StandardScaler()\n",
    "pollutant_columns = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25'] \n",
    "pollutant_columns = pollutant_columns + [f'{col}_lag_{lag}' for col in pollutant_columns for lag in lags]\n",
    "train_df[pollutant_columns] = scaler.fit_transform(train_df[pollutant_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tavg  tmin  tmax  prcp  snow   wdir  wspd  wpgt    pres  tsun\n",
      "time                                                                     \n",
      "2020-01-02   7.3   4.3   9.2   1.3   NaN  186.0   8.1  35.0  1026.4   NaN\n",
      "2020-01-03   9.8   8.5  12.2   0.5   NaN  249.0  15.7  41.0  1024.8   NaN\n",
      "2020-01-04   7.3   4.3  11.0   0.3   NaN  309.0   9.7  33.0  1035.8   NaN\n",
      "2020-01-05   7.8   7.1   9.1   0.0   NaN  265.0   5.4  21.0  1036.3   NaN\n",
      "2020-01-06   4.7   1.6   8.3   0.0   NaN  180.0   9.5  55.0  1027.6   NaN\n"
     ]
    }
   ],
   "source": [
    "# Adding External Weather Data\n",
    "\n",
    "# Define the time period for which we need the data\n",
    "start_date = train_df.index.min()\n",
    "end_date = train_df.index.max()\n",
    "\n",
    "# Define the location (Paris coordinates)\n",
    "paris = Point(48.8566, 2.3522)\n",
    "\n",
    "# Fetch daily historical data for Paris from Meteostat\n",
    "weather_data = Daily(paris, start_date, end_date)\n",
    "weather_data = weather_data.fetch()\n",
    "\n",
    "# Display the first few rows of the weather data\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in `weather_data` to use the date as a regular column\n",
    "weather_data = weather_data.reset_index()\n",
    "weather_data['weather_time'] = weather_data['time'].dt.floor('D')  # Convert to daily frequency\n",
    "\n",
    "train_df = train_df.reset_index()\n",
    "train_df['air_quality_time'] = train_df['id'].dt.floor('D')\n",
    "\n",
    "# Merge the weather data with your air quality data\n",
    "merged_df = pd.merge(train_df, weather_data, left_on='air_quality_time', right_on='weather_time', how='left')\n",
    "\n",
    "# Drop unnecessary columns and clean up\n",
    "merged_df = merged_df.drop(columns=['air_quality_time', 'weather_time'])\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "merged_df.to_csv('../data/train_with_weather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40967, 37)\n",
      "y_train shape: (40967, 5)\n",
      "X_test shape: (504, 6)\n"
     ]
    }
   ],
   "source": [
    "# Target Variable Extraction\n",
    "\n",
    "train_processed = pd.read_csv('../data/train_with_weather.csv')\n",
    "\n",
    "target_columns = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']\n",
    "\n",
    "# Separate features and target for training\n",
    "X_train = train_processed.drop(columns=target_columns)\n",
    "y_train = train_processed[target_columns]\n",
    "\n",
    "# Test set only has features, as targets are unknown\n",
    "X_test = test_df.reset_index()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to do the same preprocessing for X_test to get the same columns as X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
