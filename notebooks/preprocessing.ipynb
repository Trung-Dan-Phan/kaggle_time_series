{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install meteostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing\n",
    "\n",
    "#### Date and Time Features\n",
    "- [ ] **Extract Date Features**: Extract year, month, day, and hour from the timestamp for each entry.\n",
    "- [ ] **Cyclic Features**: Add cyclic features for hour and month using sine and cosine transformations.\n",
    "\n",
    "#### Lag and Rolling Features\n",
    "- [ ] **Lagged Values**: Create lagged features for each pollutant (1, 24, 168 hours to capture short, daily, and weekly patterns).\n",
    "- [ ] **Rolling Statistics**: Add rolling mean, rolling standard deviation, and rolling min/max features to smooth trends.\n",
    "\n",
    "#### Handle Missing Values\n",
    "- [ ] **Impute Missing Values**: Choose and implement a strategy (e.g., forward-fill, backward-fill, interpolation) for missing data in each pollutant.\n",
    "- [ ] **Drop or Flag Missing Rows**: If a significant portion of the data is missing, consider dropping those rows or creating an indicator feature to mark them.\n",
    "\n",
    "#### External Weather Data (Optional but Recommended)\n",
    "- [ ] **Incorporate Weather Data**: If available, obtain weather data for Paris (temperature, humidity, wind speed).\n",
    "- [ ] **Merge with Pollutant Data**: Align weather data with your time series dataset based on the timestamp.\n",
    "\n",
    "#### Data Scaling\n",
    "- [ ] **Scale Features**: Use standard scaling or min-max scaling for numerical features, especially for pollutants and weather variables (if any).\n",
    "\n",
    "#### Target Preparation\n",
    "- [ ] **Prepare Target Variables**: Shift the pollutant columns to create target variables aligned with the forecasting horizon (e.g., next hour, next day).\n",
    "- [ ] **Split Data**: Divide the data into training and validation sets, ensuring the split respects the time sequence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "# Replace inf/-inf with NaN (if any)\n",
    "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Impute missing values (using median as it's robust to outliers)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "train_df = pd.DataFrame(imputer.fit_transform(train_df), columns=train_df.columns, index=train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Date Information\n",
    "for df in [train_df, test_df]:\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['hour'] = df.index.hour\n",
    "\n",
    "# Create Lag Features (e.g., lags of 1, 3, 6, and 24 hours)\n",
    "lags = [1, 3, 6, 24]\n",
    "for lag in lags:\n",
    "    for col in ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']:\n",
    "        train_df[f'{col}_lag_{lag}'] = train_df[col].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values introduced by lags (at the beginning of the data)\n",
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling/Normalization\n",
    "\n",
    "# Select features for scaling (all pollutant columns and lags)\n",
    "scaler = StandardScaler()\n",
    "pollutant_columns = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25'] \n",
    "pollutant_columns = pollutant_columns + [f'{col}_lag_{lag}' for col in pollutant_columns for lag in lags]\n",
    "train_df[pollutant_columns] = scaler.fit_transform(train_df[pollutant_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tavg  tmin  tmax  prcp  snow   wdir  wspd  wpgt    pres  tsun\n",
      "time                                                                     \n",
      "2020-01-02   7.3   4.3   9.2   1.3   NaN  186.0   8.1  35.0  1026.4   NaN\n",
      "2020-01-03   9.8   8.5  12.2   0.5   NaN  249.0  15.7  41.0  1024.8   NaN\n",
      "2020-01-04   7.3   4.3  11.0   0.3   NaN  309.0   9.7  33.0  1035.8   NaN\n",
      "2020-01-05   7.8   7.1   9.1   0.0   NaN  265.0   5.4  21.0  1036.3   NaN\n",
      "2020-01-06   4.7   1.6   8.3   0.0   NaN  180.0   9.5  55.0  1027.6   NaN\n"
     ]
    }
   ],
   "source": [
    "# Adding External Weather Data\n",
    "\n",
    "# Define the time period for which we need the data\n",
    "start_date = train_df.index.min()\n",
    "end_date = train_df.index.max()\n",
    "\n",
    "# Define the location (Paris coordinates)\n",
    "paris = Point(48.8566, 2.3522)\n",
    "\n",
    "# Fetch daily historical data for Paris from Meteostat\n",
    "weather_data = Daily(paris, start_date, end_date)\n",
    "weather_data = weather_data.fetch()\n",
    "\n",
    "# Display the first few rows of the weather data\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index in `weather_data` to use the date as a regular column\n",
    "weather_data = weather_data.reset_index()\n",
    "weather_data['weather_time'] = weather_data['time'].dt.floor('D')  # Convert to daily frequency\n",
    "\n",
    "train_df = train_df.reset_index()\n",
    "train_df['air_quality_time'] = train_df['id'].dt.floor('D')\n",
    "\n",
    "# Merge the weather data with your air quality data\n",
    "merged_df = pd.merge(train_df, weather_data, left_on='air_quality_time', right_on='weather_time', how='left')\n",
    "\n",
    "# Drop unnecessary columns and clean up\n",
    "merged_df = merged_df.drop(columns=['air_quality_time', 'weather_time'])\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "merged_df.to_csv('../data/train_with_weather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (40967, 37)\n",
      "y_train shape: (40967, 5)\n",
      "X_test shape: (504, 6)\n"
     ]
    }
   ],
   "source": [
    "# Target Variable Extraction\n",
    "\n",
    "train_processed = pd.read_csv('../data/train_with_weather.csv')\n",
    "\n",
    "target_columns = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']\n",
    "\n",
    "# Separate features and target for training\n",
    "X_train = train_processed.drop(columns=target_columns)\n",
    "y_train = train_processed[target_columns]\n",
    "\n",
    "# Test set only has features, as targets are unknown\n",
    "X_test = test_df.reset_index()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to do the same preprocessing for X_test to get the same columns as X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
